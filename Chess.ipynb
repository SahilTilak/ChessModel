{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4a81605c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import berserk\n",
    "import chessdotcom\n",
    "from stockfish import Stockfish\n",
    "import chess\n",
    "import chess.pgn\n",
    "import io\n",
    "import chess.engine\n",
    "import os\n",
    "import pandas as pd\n",
    "import random\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "df38dedb",
   "metadata": {},
   "outputs": [],
   "source": [
    "stockfish = Stockfish()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "id": "eefe1e4e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_titled_tuesday_matches():\n",
    "    all_games = []\n",
    "    usernames = []\n",
    "    blitz_leaderboard = chessdotcom.client.get_leaderboards().json['leaderboards']['live_blitz']\n",
    "    for name in blitz_leaderboard:\n",
    "        usernames.append(name['username'])\n",
    "    for name in usernames:\n",
    "        games = chessdotcom.client.get_player_game_archives(username = name)\n",
    "        recent_games = requests.get(games.json['archives'][-1]).json()['games']\n",
    "        for game in recent_games:\n",
    "            if(game['time_control'] != '180'):\n",
    "                continue\n",
    "            if(game['white']['username'] != name):\n",
    "                try:\n",
    "                    other_title = chessdotcom.client.get_player_profile(username = game['white']['username']).json['player']['title']\n",
    "                except KeyError:\n",
    "                    other_title = 'None'\n",
    "            else:\n",
    "                try:\n",
    "                    other_title = chessdotcom.client.get_player_profile(username = game['black']['username']).json['player']['title']\n",
    "                except KeyError:\n",
    "                    other_title = 'None'\n",
    "            if(other_title == 'None'):\n",
    "                continue\n",
    "            all_games.append(game['pgn'])\n",
    "    return all_games\n",
    "            \n",
    "                \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 819,
   "id": "fda45d82",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_df():\n",
    "    engine = chess.engine.SimpleEngine.popen_uci(\"/opt/homebrew/Cellar/stockfish/15/bin/stockfish\")\n",
    "    df = pd.DataFrame()\n",
    "    stockfish = []\n",
    "    move_num = []\n",
    "    white_time = []\n",
    "    black_time = []\n",
    "    white_name = []\n",
    "    black_name = []\n",
    "    white_elo = []\n",
    "    black_elo = []\n",
    "    result = []\n",
    "    to_move = []\n",
    "    games = chessdotcom.client.get_tournament_round(url_id= 'late-titled-tuesday-blitz-august-02-2022-3288264/11', round_num = 1).json['tournament_round']['games']\n",
    "    games2 = chessdotcom.client.get_tournament_round(url_id= 'early-titled-tuesday-blitz-august-02-2022-3288263/11', round_num = 1).json['tournament_round']['games']\n",
    "    count = 0\n",
    "    for one_game in games:\n",
    "        pgn = io.StringIO(one_game['pgn'])\n",
    "        game = chess.pgn.read_game(pgn)\n",
    "        if 'agreement' in game.headers['Termination']:\n",
    "            continue\n",
    "        w_name =(game.headers['White'])\n",
    "        b_name = (game.headers['Black'])\n",
    "        w_elo = (game.headers['WhiteElo'])\n",
    "        b_elo = (game.headers['BlackElo'])\n",
    "        game_result =(game.headers['Result'])\n",
    "        moves = 0\n",
    "        move_num.append(moves)\n",
    "        white_time_move = 180\n",
    "        black_time_move = 180\n",
    "        white_time.append(white_time_move)\n",
    "        black_time.append(black_time_move)\n",
    "        to_move.append('white')\n",
    "        stockfish.append(40)\n",
    "        result.append(game_result)\n",
    "        white_name.append(w_name)\n",
    "        black_name.append(b_name)\n",
    "        white_elo.append(w_elo)\n",
    "        black_elo.append(b_elo)\n",
    "        moves = moves + 1\n",
    "        \n",
    "        while not game.is_end():\n",
    "            node = game.variations[0]\n",
    "            if (moves % 2 == 0):\n",
    "                black_time_move = float(node.clock())\n",
    "            else:\n",
    "                white_time_move = float(node.clock())\n",
    "            board = game.board() #print the board if you want, to make sure\n",
    "            fen = board.fen()\n",
    "            if(random.random() < .1):\n",
    "                move_num.append(moves)\n",
    "                result.append(game_result)\n",
    "                white_name.append(w_name)\n",
    "                black_name.append(b_name)\n",
    "                white_elo.append(w_elo)\n",
    "                black_elo.append(b_elo)\n",
    "                stockfish.append((engine.analyse(chess.Board(fen), chess.engine.Limit(time = .075))['score'].white().score()))\n",
    "                white_time.append(white_time_move)\n",
    "                black_time.append(black_time_move)\n",
    "                if(moves % 2 == 0):\n",
    "                    to_move.append(\"white\")\n",
    "                else:\n",
    "                    to_move.append(\"black\")\n",
    "            game = node\n",
    "            moves = moves+1\n",
    "    \n",
    "        count = count + 1\n",
    "        print(count)\n",
    "    for one_game in games2:\n",
    "        pgn = io.StringIO(one_game['pgn'])\n",
    "        game = chess.pgn.read_game(pgn)\n",
    "        if 'agreement' in game.headers['Termination']:\n",
    "            continue\n",
    "        w_name =(game.headers['White'])\n",
    "        b_name = (game.headers['Black'])\n",
    "        w_elo = (game.headers['WhiteElo'])\n",
    "        b_elo = (game.headers['BlackElo'])\n",
    "        game_result =(game.headers['Result'])\n",
    "        moves = 0\n",
    "        move_num.append(moves)\n",
    "        white_time_move = 180\n",
    "        black_time_move = 180\n",
    "        white_time.append(white_time_move)\n",
    "        black_time.append(black_time_move)\n",
    "        to_move.append('white')\n",
    "        stockfish.append(40)\n",
    "        result.append(game_result)\n",
    "        white_name.append(w_name)\n",
    "        black_name.append(b_name)\n",
    "        white_elo.append(w_elo)\n",
    "        black_elo.append(b_elo)\n",
    "        moves = moves + 1\n",
    "        \n",
    "        while not game.is_end():\n",
    "            node = game.variations[0]\n",
    "            if (moves % 2 == 0):\n",
    "                black_time_move = float(node.clock())\n",
    "            else:\n",
    "                white_time_move = float(node.clock())\n",
    "            board = game.board() #print the board if you want, to make sure\n",
    "            fen = board.fen()\n",
    "            if(random.random() < .1):\n",
    "                move_num.append(moves)\n",
    "                result.append(game_result)\n",
    "                white_name.append(w_name)\n",
    "                black_name.append(b_name)\n",
    "                white_elo.append(w_elo)\n",
    "                black_elo.append(b_elo)\n",
    "                stockfish.append((engine.analyse(chess.Board(fen), chess.engine.Limit(time = .15))['score'].white().score()))\n",
    "                white_time.append(white_time_move)\n",
    "                black_time.append(black_time_move)\n",
    "                if(moves % 2 == 0):\n",
    "                    to_move.append(\"white\")\n",
    "                else:\n",
    "                    to_move.append(\"black\")\n",
    "            game = node\n",
    "            moves = moves+1\n",
    "    \n",
    "        count = count + 1\n",
    "        print(count)\n",
    "    df['white_name'] = white_name\n",
    "    df['black_name'] = black_name\n",
    "    df['white_elo'] = white_elo\n",
    "    df['black_elo'] = black_elo\n",
    "    df['result'] = result\n",
    "    df['stockfish'] = stockfish\n",
    "    df['move_num'] = move_num\n",
    "    df['white_time'] = white_time\n",
    "    df['black_time'] = black_time\n",
    "    df['to_move'] = to_move\n",
    "    return df\n",
    "                \n",
    "                         \n",
    "                        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 822,
   "id": "9c3004a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def white_time(row):\n",
    "    if row['to_move'] == 'white':\n",
    "        return row['white_time']\n",
    "    else:\n",
    "        return row['black_time']\n",
    "def black_time(row):\n",
    "    if row['to_move'] == 'black':\n",
    "        return row['white_time']\n",
    "    else:\n",
    "        return row['black_time']\n",
    "def result_encode(row):\n",
    "    if row['result'] == '1-0':\n",
    "        return 2\n",
    "    elif row['result'] == '0-1':\n",
    "        return 0\n",
    "    else:\n",
    "        return 1\n",
    "\n",
    "df['to_move_time'] = df.apply(white_time, axis=1)\n",
    "df['opp_time'] = df.apply(black_time, axis=1)\n",
    "df['encoded_result'] = df.apply(result_encode, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 719,
   "id": "1fcefbcf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df2['encoded_result'] = df2.apply(result_encode, axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 823,
   "id": "93c7a8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['white_elo'] = pd.to_numeric(df['white_elo'])\n",
    "df['black_elo'] = pd.to_numeric(df['black_elo'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 721,
   "id": "c0c080f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = df2[['white_elo', 'black_elo', 'stockfish', 'move_num', 'white_time', 'black_time']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 724,
   "id": "5a59a4dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "y2 = df2[['encoded_result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 725,
   "id": "27841f18",
   "metadata": {},
   "outputs": [],
   "source": [
    "Y = pd.concat([Y, y2], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 722,
   "id": "7faecc0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = pd.concat([X, x2], ignore_index = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 726,
   "id": "72ee7f7d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>encoded_result</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27516</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27517</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27518</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27519</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27520</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>27521 rows Ã— 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       encoded_result\n",
       "0                   2\n",
       "1                   2\n",
       "2                   2\n",
       "3                   2\n",
       "4                   2\n",
       "...               ...\n",
       "27516               0\n",
       "27517               0\n",
       "27518               0\n",
       "27519               0\n",
       "27520               0\n",
       "\n",
       "[27521 rows x 1 columns]"
      ]
     },
     "execution_count": 726,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 832,
   "id": "9b074b6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from xgboost import XGBClassifier\n",
    "from xgboost import plot_importance\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 824,
   "id": "cc61779e",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df[['white_elo', 'black_elo', 'stockfish', 'white_time', 'black_time']]\n",
    "Y = df[['encoded_result']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 868,
   "id": "20d7644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_size = .25\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, Y, test_size=test_size, shuffle = False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 886,
   "id": "a8189c55",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting 5 folds for each of 24 candidates, totalling 120 fits\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.715 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.665 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.652 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.688 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.664 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.725 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.673 total time=   1.9s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.658 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.709 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.668 total time=   2.0s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.726 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.680 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.672 total time=   0.7s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.714 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.668 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.735 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.693 total time=   2.8s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.671 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.722 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.686 total time=   2.7s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.736 total time=   1.1s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.687 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.672 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.717 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.680 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.732 total time=   2.1s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.687 total time=   2.1s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.657 total time=   2.1s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.724 total time=   2.2s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.687 total time=   2.3s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.715 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.665 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.652 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.688 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.664 total time=   1.0s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.725 total time=   2.5s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.673 total time=   2.6s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.658 total time=   2.5s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.709 total time=   2.3s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.668 total time=   2.8s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.726 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.680 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.672 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.714 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.668 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.736 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.694 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.669 total time=   2.0s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.722 total time=   2.1s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.684 total time=   2.1s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.736 total time=   0.7s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.688 total time=   0.7s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.671 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.717 total time=   0.7s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.681 total time=   0.7s\n",
      "[CV 1/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.729 total time=   2.0s\n",
      "[CV 2/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.692 total time=   2.1s\n",
      "[CV 3/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.659 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.723 total time=   2.4s\n",
      "[CV 5/5] END colsample_bytree=0.3, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.686 total time=   2.4s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.723 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.665 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.661 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.706 total time=   0.8s\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[CV 5/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.680 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.736 total time=   2.5s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.691 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.684 total time=   2.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.719 total time=   3.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.682 total time=   2.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.737 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.690 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.678 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.723 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.687 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.737 total time=   2.4s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.695 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.673 total time=   2.3s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.726 total time=   2.4s\n",
      "[CV 5/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.694 total time=   2.5s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.740 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.695 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.681 total time=   0.9s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.726 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.695 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.735 total time=   2.6s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.693 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.656 total time=   2.4s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.722 total time=   2.6s\n",
      "[CV 5/5] END colsample_bytree=0.5, gamma=0, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.683 total time=   2.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.724 total time=   0.8s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.665 total time=   0.9s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.661 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.706 total time=   1.0s\n",
      "[CV 5/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=100;, score=0.680 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.736 total time=   2.7s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.691 total time=   2.6s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.684 total time=   2.5s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.719 total time=   2.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.01, max_depth=3, n_estimators=300;, score=0.682 total time=   2.6s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.737 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.690 total time=   0.8s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.678 total time=   0.8s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.723 total time=   0.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=100;, score=0.687 total time=   0.9s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.736 total time=   2.3s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.696 total time=   2.3s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.674 total time=   2.7s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.725 total time=   2.9s\n",
      "[CV 5/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.05, max_depth=3, n_estimators=300;, score=0.693 total time=   2.7s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.741 total time=   0.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.695 total time=   1.0s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.680 total time=   1.0s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.725 total time=   0.8s\n",
      "[CV 5/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=100;, score=0.694 total time=   0.8s\n",
      "[CV 1/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.735 total time=   2.9s\n",
      "[CV 2/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.695 total time=   2.4s\n",
      "[CV 3/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.652 total time=   2.6s\n",
      "[CV 4/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.724 total time=   2.5s\n",
      "[CV 5/5] END colsample_bytree=0.5, gamma=0.1, learning_rate=0.1, max_depth=3, n_estimators=300;, score=0.684 total time=   2.3s\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5,\n",
       "             estimator=XGBClassifier(base_score=None, booster=None,\n",
       "                                     callbacks=None, colsample_bylevel=None,\n",
       "                                     colsample_bynode=None,\n",
       "                                     colsample_bytree=None,\n",
       "                                     early_stopping_rounds=None,\n",
       "                                     enable_categorical=False, eval_metric=None,\n",
       "                                     gamma=None, gpu_id=None, grow_policy=None,\n",
       "                                     importance_type=None,\n",
       "                                     interaction_constraints=None,\n",
       "                                     learning_rate=None, max_bin=None,\n",
       "                                     max_ca...\n",
       "                                     max_leaves=None, min_child_weight=None,\n",
       "                                     missing=nan, monotone_constraints=None,\n",
       "                                     n_estimators=100, n_jobs=None,\n",
       "                                     num_parallel_tree=None, predictor=None,\n",
       "                                     random_state=None, reg_alpha=None,\n",
       "                                     reg_lambda=None, ...),\n",
       "             param_grid={'colsample_bytree': [0.3, 0.5], 'gamma': [0, 0.1],\n",
       "                         'learning_rate': [0.01, 0.05, 0.1], 'max_depth': [3],\n",
       "                         'n_estimators': [100, 300]},\n",
       "             scoring='accuracy', verbose=3)"
      ]
     },
     "execution_count": 886,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier()\n",
    "params = { 'max_depth': [3],\n",
    "           'learning_rate': [0.01, .05, .1],\n",
    "           'n_estimators': [100, 300],\n",
    "           'colsample_bytree': [0.3, .5],\n",
    "           'gamma': [0, .1]\n",
    "         }\n",
    "\n",
    "clf = GridSearchCV(estimator=model, \n",
    "                   param_grid=params,\n",
    "                   scoring='accuracy', \n",
    "                   verbose=3, cv = 5)\n",
    "clf.fit(X_train, y_train)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 887,
   "id": "cbdfb2ce",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'colsample_bytree': 0.5,\n",
       " 'gamma': 0,\n",
       " 'learning_rate': 0.1,\n",
       " 'max_depth': 3,\n",
       " 'n_estimators': 100}"
      ]
     },
     "execution_count": 887,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 892,
   "id": "54d808c9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "XGBClassifier(base_score=0.5, booster='gbtree', callbacks=None,\n",
       "              colsample_bylevel=1, colsample_bynode=1, colsample_bytree=0.5,\n",
       "              early_stopping_rounds=None, enable_categorical=False,\n",
       "              eval_metric=None, gamma=0, gpu_id=-1, grow_policy='depthwise',\n",
       "              importance_type=None, interaction_constraints='',\n",
       "              learning_rate=0.1, max_bin=256, max_cat_to_onehot=4,\n",
       "              max_delta_step=0, max_depth=5, max_leaves=0, min_child_weight=1,\n",
       "              missing=nan, monotone_constraints='()', n_estimators=100,\n",
       "              n_jobs=0, num_parallel_tree=1, objective='multi:softprob',\n",
       "              predictor='auto', random_state=0, reg_alpha=0, ...)"
      ]
     },
     "execution_count": 892,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model = XGBClassifier(colsample_bytree= .5, max_depth = 5, n_estimators = 100, learning_rate = .1)\n",
    "model.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 827,
   "id": "40f45842",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<AxesSubplot:title={'center':'Feature importance'}, xlabel='F score', ylabel='Features'>"
      ]
     },
     "execution_count": 827,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "plot_importance(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 747,
   "id": "85153a40",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2    9901\n",
       "0    9375\n",
       "1    2740\n",
       "Name: encoded_result, dtype: int64"
      ]
     },
     "execution_count": 747,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train['encoded_result'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 893,
   "id": "d6966676",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = model.predict(X_test)\n",
    "predictions = [round(value) for value in y_pred]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 881,
   "id": "f57be8fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.14062047, 0.13749321, 0.72188634],\n",
       "       [0.08667517, 0.09583018, 0.8174947 ],\n",
       "       [0.08133675, 0.12851472, 0.79014856],\n",
       "       ...,\n",
       "       [0.8865587 , 0.03334993, 0.08009129],\n",
       "       [0.8932891 , 0.03360311, 0.07310776],\n",
       "       [0.9115961 , 0.03429177, 0.05411205]], dtype=float32)"
      ]
     },
     "execution_count": 881,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 692,
   "id": "f3d50aa7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       0.121404\n",
       "1       0.124293\n",
       "2       0.142365\n",
       "3       0.149143\n",
       "4       0.109467\n",
       "          ...   \n",
       "2492    0.533708\n",
       "2493    0.595113\n",
       "2494    0.804016\n",
       "2495    0.973854\n",
       "2496    0.974958\n",
       "Name: 2, Length: 2497, dtype: float32"
      ]
     },
     "execution_count": 692,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.Series(pd.DataFrame(np.array(model.predict_proba(X_test)))[2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 864,
   "id": "50e983c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "white = (pd.Series(pd.DataFrame(np.array(model.predict_proba(X_test)))[2]))\n",
    "draw = (pd.Series(pd.DataFrame(np.array(model.predict_proba(X_test)))[1]))\n",
    "black = (pd.Series(pd.DataFrame(np.array(model.predict_proba(X_test)))[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 865,
   "id": "6ddd420d",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_test = X_test.assign(white=white.values)\n",
    "X_test = X_test.assign(draw=draw.values)\n",
    "X_test = X_test.assign(black=black.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 866,
   "id": "ed89a96e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>white_elo</th>\n",
       "      <th>black_elo</th>\n",
       "      <th>stockfish</th>\n",
       "      <th>white_time</th>\n",
       "      <th>black_time</th>\n",
       "      <th>white</th>\n",
       "      <th>draw</th>\n",
       "      <th>black</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>27817</th>\n",
       "      <td>2688</td>\n",
       "      <td>2894</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.4</td>\n",
       "      <td>0.427794</td>\n",
       "      <td>0.165557</td>\n",
       "      <td>0.406649</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27818</th>\n",
       "      <td>2688</td>\n",
       "      <td>2894</td>\n",
       "      <td>-11.0</td>\n",
       "      <td>6.3</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.360202</td>\n",
       "      <td>0.207687</td>\n",
       "      <td>0.432110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27819</th>\n",
       "      <td>2688</td>\n",
       "      <td>2894</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>7.1</td>\n",
       "      <td>2.9</td>\n",
       "      <td>0.371852</td>\n",
       "      <td>0.214405</td>\n",
       "      <td>0.413743</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27820</th>\n",
       "      <td>2688</td>\n",
       "      <td>2894</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>7.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.360202</td>\n",
       "      <td>0.207687</td>\n",
       "      <td>0.432110</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27821</th>\n",
       "      <td>2688</td>\n",
       "      <td>2894</td>\n",
       "      <td>-15.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>0.351370</td>\n",
       "      <td>0.221396</td>\n",
       "      <td>0.427234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27822</th>\n",
       "      <td>2688</td>\n",
       "      <td>2894</td>\n",
       "      <td>-17.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.2</td>\n",
       "      <td>0.428088</td>\n",
       "      <td>0.165671</td>\n",
       "      <td>0.406240</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27823</th>\n",
       "      <td>2688</td>\n",
       "      <td>2894</td>\n",
       "      <td>-13.0</td>\n",
       "      <td>6.4</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.369482</td>\n",
       "      <td>0.213038</td>\n",
       "      <td>0.417480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27824</th>\n",
       "      <td>2688</td>\n",
       "      <td>2894</td>\n",
       "      <td>-19.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.416991</td>\n",
       "      <td>0.161376</td>\n",
       "      <td>0.421633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27825</th>\n",
       "      <td>2688</td>\n",
       "      <td>2894</td>\n",
       "      <td>-21.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.287703</td>\n",
       "      <td>0.171021</td>\n",
       "      <td>0.541276</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27826</th>\n",
       "      <td>2688</td>\n",
       "      <td>2894</td>\n",
       "      <td>-448.0</td>\n",
       "      <td>2.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>0.179305</td>\n",
       "      <td>0.124212</td>\n",
       "      <td>0.696483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27827</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>40.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.554202</td>\n",
       "      <td>0.139174</td>\n",
       "      <td>0.306624</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27828</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>14.0</td>\n",
       "      <td>179.3</td>\n",
       "      <td>174.1</td>\n",
       "      <td>0.548795</td>\n",
       "      <td>0.145420</td>\n",
       "      <td>0.305785</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27829</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>1.0</td>\n",
       "      <td>179.3</td>\n",
       "      <td>174.7</td>\n",
       "      <td>0.450347</td>\n",
       "      <td>0.239661</td>\n",
       "      <td>0.309992</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27830</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>9.0</td>\n",
       "      <td>179.2</td>\n",
       "      <td>165.0</td>\n",
       "      <td>0.488215</td>\n",
       "      <td>0.202813</td>\n",
       "      <td>0.308972</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27831</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>18.0</td>\n",
       "      <td>177.1</td>\n",
       "      <td>164.0</td>\n",
       "      <td>0.548960</td>\n",
       "      <td>0.145162</td>\n",
       "      <td>0.305877</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27832</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>-3.0</td>\n",
       "      <td>158.8</td>\n",
       "      <td>159.8</td>\n",
       "      <td>0.492397</td>\n",
       "      <td>0.176368</td>\n",
       "      <td>0.331235</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27833</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>-22.0</td>\n",
       "      <td>142.6</td>\n",
       "      <td>148.3</td>\n",
       "      <td>0.496131</td>\n",
       "      <td>0.164202</td>\n",
       "      <td>0.339667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27834</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>-80.0</td>\n",
       "      <td>142.6</td>\n",
       "      <td>138.8</td>\n",
       "      <td>0.461751</td>\n",
       "      <td>0.152250</td>\n",
       "      <td>0.385999</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27835</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>11.0</td>\n",
       "      <td>123.1</td>\n",
       "      <td>117.4</td>\n",
       "      <td>0.544999</td>\n",
       "      <td>0.142768</td>\n",
       "      <td>0.312234</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27836</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>317.0</td>\n",
       "      <td>102.1</td>\n",
       "      <td>99.1</td>\n",
       "      <td>0.671131</td>\n",
       "      <td>0.134052</td>\n",
       "      <td>0.194817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27837</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>310.0</td>\n",
       "      <td>101.6</td>\n",
       "      <td>99.1</td>\n",
       "      <td>0.671131</td>\n",
       "      <td>0.134052</td>\n",
       "      <td>0.194817</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27838</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>276.0</td>\n",
       "      <td>88.5</td>\n",
       "      <td>84.8</td>\n",
       "      <td>0.676270</td>\n",
       "      <td>0.139802</td>\n",
       "      <td>0.183928</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27839</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>60.0</td>\n",
       "      <td>72.4</td>\n",
       "      <td>40.1</td>\n",
       "      <td>0.546871</td>\n",
       "      <td>0.161759</td>\n",
       "      <td>0.291371</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27840</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>69.0</td>\n",
       "      <td>17.6</td>\n",
       "      <td>15.8</td>\n",
       "      <td>0.507497</td>\n",
       "      <td>0.213257</td>\n",
       "      <td>0.279245</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27841</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>55.0</td>\n",
       "      <td>15.2</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.503516</td>\n",
       "      <td>0.215651</td>\n",
       "      <td>0.280833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27842</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>54.0</td>\n",
       "      <td>15.1</td>\n",
       "      <td>12.0</td>\n",
       "      <td>0.503516</td>\n",
       "      <td>0.215651</td>\n",
       "      <td>0.280833</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27843</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>53.0</td>\n",
       "      <td>15.1</td>\n",
       "      <td>12.4</td>\n",
       "      <td>0.502739</td>\n",
       "      <td>0.215318</td>\n",
       "      <td>0.281943</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27844</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>53.0</td>\n",
       "      <td>9.6</td>\n",
       "      <td>8.6</td>\n",
       "      <td>0.478643</td>\n",
       "      <td>0.272663</td>\n",
       "      <td>0.248695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27845</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>52.0</td>\n",
       "      <td>9.7</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.478643</td>\n",
       "      <td>0.272663</td>\n",
       "      <td>0.248695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27846</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>11.0</td>\n",
       "      <td>0.476002</td>\n",
       "      <td>0.259829</td>\n",
       "      <td>0.264169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27847</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>50.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>11.3</td>\n",
       "      <td>0.451694</td>\n",
       "      <td>0.246561</td>\n",
       "      <td>0.301746</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27848</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>47.0</td>\n",
       "      <td>5.3</td>\n",
       "      <td>8.8</td>\n",
       "      <td>0.440084</td>\n",
       "      <td>0.273697</td>\n",
       "      <td>0.286218</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27849</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.463232</td>\n",
       "      <td>0.289084</td>\n",
       "      <td>0.247685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27850</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.456312</td>\n",
       "      <td>0.284765</td>\n",
       "      <td>0.258922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27851</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.5</td>\n",
       "      <td>7.6</td>\n",
       "      <td>0.456312</td>\n",
       "      <td>0.284765</td>\n",
       "      <td>0.258922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27852</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>50.0</td>\n",
       "      <td>6.8</td>\n",
       "      <td>7.8</td>\n",
       "      <td>0.456312</td>\n",
       "      <td>0.284765</td>\n",
       "      <td>0.258922</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27853</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>47.0</td>\n",
       "      <td>7.3</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.456280</td>\n",
       "      <td>0.283770</td>\n",
       "      <td>0.259951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27854</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>47.0</td>\n",
       "      <td>7.8</td>\n",
       "      <td>8.4</td>\n",
       "      <td>0.456280</td>\n",
       "      <td>0.283770</td>\n",
       "      <td>0.259951</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27855</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>0.0</td>\n",
       "      <td>6.2</td>\n",
       "      <td>9.0</td>\n",
       "      <td>0.329367</td>\n",
       "      <td>0.438185</td>\n",
       "      <td>0.232449</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27856</th>\n",
       "      <td>3032</td>\n",
       "      <td>2987</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.4</td>\n",
       "      <td>7.9</td>\n",
       "      <td>0.315847</td>\n",
       "      <td>0.415670</td>\n",
       "      <td>0.268483</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27857</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>40.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>180.0</td>\n",
       "      <td>0.476648</td>\n",
       "      <td>0.091680</td>\n",
       "      <td>0.431672</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27858</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>35.0</td>\n",
       "      <td>180.9</td>\n",
       "      <td>176.6</td>\n",
       "      <td>0.476291</td>\n",
       "      <td>0.091742</td>\n",
       "      <td>0.431967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27859</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>42.0</td>\n",
       "      <td>179.1</td>\n",
       "      <td>160.9</td>\n",
       "      <td>0.475787</td>\n",
       "      <td>0.091455</td>\n",
       "      <td>0.432759</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27860</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>68.0</td>\n",
       "      <td>145.9</td>\n",
       "      <td>131.0</td>\n",
       "      <td>0.470737</td>\n",
       "      <td>0.089022</td>\n",
       "      <td>0.440241</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27861</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>41.0</td>\n",
       "      <td>131.2</td>\n",
       "      <td>91.9</td>\n",
       "      <td>0.481432</td>\n",
       "      <td>0.098613</td>\n",
       "      <td>0.419956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27862</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>110.0</td>\n",
       "      <td>88.5</td>\n",
       "      <td>59.8</td>\n",
       "      <td>0.480937</td>\n",
       "      <td>0.107430</td>\n",
       "      <td>0.411633</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27863</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>70.0</td>\n",
       "      <td>88.5</td>\n",
       "      <td>39.8</td>\n",
       "      <td>0.476491</td>\n",
       "      <td>0.107950</td>\n",
       "      <td>0.415560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27864</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>51.0</td>\n",
       "      <td>70.5</td>\n",
       "      <td>37.4</td>\n",
       "      <td>0.477006</td>\n",
       "      <td>0.106985</td>\n",
       "      <td>0.416009</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27865</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>43.0</td>\n",
       "      <td>70.8</td>\n",
       "      <td>37.4</td>\n",
       "      <td>0.477181</td>\n",
       "      <td>0.106658</td>\n",
       "      <td>0.416162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27866</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>-101.0</td>\n",
       "      <td>20.9</td>\n",
       "      <td>21.8</td>\n",
       "      <td>0.327875</td>\n",
       "      <td>0.150532</td>\n",
       "      <td>0.521593</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27867</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>-269.0</td>\n",
       "      <td>5.9</td>\n",
       "      <td>15.6</td>\n",
       "      <td>0.255191</td>\n",
       "      <td>0.158505</td>\n",
       "      <td>0.586303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27868</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>-269.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>15.3</td>\n",
       "      <td>0.255191</td>\n",
       "      <td>0.158505</td>\n",
       "      <td>0.586303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27869</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>-538.0</td>\n",
       "      <td>3.2</td>\n",
       "      <td>10.5</td>\n",
       "      <td>0.175308</td>\n",
       "      <td>0.109886</td>\n",
       "      <td>0.714807</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27870</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>-771.0</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.159048</td>\n",
       "      <td>0.123497</td>\n",
       "      <td>0.717455</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27871</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>-694.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>0.185882</td>\n",
       "      <td>0.113287</td>\n",
       "      <td>0.700831</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27872</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>-753.0</td>\n",
       "      <td>2.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>0.160254</td>\n",
       "      <td>0.116853</td>\n",
       "      <td>0.722893</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27873</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>-556.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>4.4</td>\n",
       "      <td>0.220721</td>\n",
       "      <td>0.125413</td>\n",
       "      <td>0.653866</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27874</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>-1246.0</td>\n",
       "      <td>4.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.242174</td>\n",
       "      <td>0.073512</td>\n",
       "      <td>0.684314</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27875</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>-1256.0</td>\n",
       "      <td>3.5</td>\n",
       "      <td>2.3</td>\n",
       "      <td>0.232750</td>\n",
       "      <td>0.070929</td>\n",
       "      <td>0.696321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27876</th>\n",
       "      <td>2829</td>\n",
       "      <td>2765</td>\n",
       "      <td>-1241.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>0.208334</td>\n",
       "      <td>0.094590</td>\n",
       "      <td>0.697076</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       white_elo  black_elo  stockfish  white_time  black_time     white  \\\n",
       "27817       2688       2894      -15.0         5.7         2.4  0.427794   \n",
       "27818       2688       2894      -11.0         6.3         3.3  0.360202   \n",
       "27819       2688       2894      -13.0         7.1         2.9  0.371852   \n",
       "27820       2688       2894      -19.0         7.4         3.1  0.360202   \n",
       "27821       2688       2894      -15.0         6.8         2.7  0.351370   \n",
       "27822       2688       2894      -17.0         6.1         2.2  0.428088   \n",
       "27823       2688       2894      -13.0         6.4         3.0  0.369482   \n",
       "27824       2688       2894      -19.0         7.0         2.3  0.416991   \n",
       "27825       2688       2894      -21.0         2.0         3.3  0.287703   \n",
       "27826       2688       2894     -448.0         2.1         3.3  0.179305   \n",
       "27827       3032       2987       40.0       180.0       180.0  0.554202   \n",
       "27828       3032       2987       14.0       179.3       174.1  0.548795   \n",
       "27829       3032       2987        1.0       179.3       174.7  0.450347   \n",
       "27830       3032       2987        9.0       179.2       165.0  0.488215   \n",
       "27831       3032       2987       18.0       177.1       164.0  0.548960   \n",
       "27832       3032       2987       -3.0       158.8       159.8  0.492397   \n",
       "27833       3032       2987      -22.0       142.6       148.3  0.496131   \n",
       "27834       3032       2987      -80.0       142.6       138.8  0.461751   \n",
       "27835       3032       2987       11.0       123.1       117.4  0.544999   \n",
       "27836       3032       2987      317.0       102.1        99.1  0.671131   \n",
       "27837       3032       2987      310.0       101.6        99.1  0.671131   \n",
       "27838       3032       2987      276.0        88.5        84.8  0.676270   \n",
       "27839       3032       2987       60.0        72.4        40.1  0.546871   \n",
       "27840       3032       2987       69.0        17.6        15.8  0.507497   \n",
       "27841       3032       2987       55.0        15.2        12.0  0.503516   \n",
       "27842       3032       2987       54.0        15.1        12.0  0.503516   \n",
       "27843       3032       2987       53.0        15.1        12.4  0.502739   \n",
       "27844       3032       2987       53.0         9.6         8.6  0.478643   \n",
       "27845       3032       2987       52.0         9.7         8.8  0.478643   \n",
       "27846       3032       2987       50.0         6.0        11.0  0.476002   \n",
       "27847       3032       2987       50.0         5.2        11.3  0.451694   \n",
       "27848       3032       2987       47.0         5.3         8.8  0.440084   \n",
       "27849       3032       2987       50.0         6.1         8.4  0.463232   \n",
       "27850       3032       2987       50.0         6.8         7.6  0.456312   \n",
       "27851       3032       2987       50.0         6.5         7.6  0.456312   \n",
       "27852       3032       2987       50.0         6.8         7.8  0.456312   \n",
       "27853       3032       2987       47.0         7.3         8.4  0.456280   \n",
       "27854       3032       2987       47.0         7.8         8.4  0.456280   \n",
       "27855       3032       2987        0.0         6.2         9.0  0.329367   \n",
       "27856       3032       2987        2.0         5.4         7.9  0.315847   \n",
       "27857       2829       2765       40.0       180.0       180.0  0.476648   \n",
       "27858       2829       2765       35.0       180.9       176.6  0.476291   \n",
       "27859       2829       2765       42.0       179.1       160.9  0.475787   \n",
       "27860       2829       2765       68.0       145.9       131.0  0.470737   \n",
       "27861       2829       2765       41.0       131.2        91.9  0.481432   \n",
       "27862       2829       2765      110.0        88.5        59.8  0.480937   \n",
       "27863       2829       2765       70.0        88.5        39.8  0.476491   \n",
       "27864       2829       2765       51.0        70.5        37.4  0.477006   \n",
       "27865       2829       2765       43.0        70.8        37.4  0.477181   \n",
       "27866       2829       2765     -101.0        20.9        21.8  0.327875   \n",
       "27867       2829       2765     -269.0         5.9        15.6  0.255191   \n",
       "27868       2829       2765     -269.0         6.0        15.3  0.255191   \n",
       "27869       2829       2765     -538.0         3.2        10.5  0.175308   \n",
       "27870       2829       2765     -771.0         2.8         4.0  0.159048   \n",
       "27871       2829       2765     -694.0         2.0         3.5  0.185882   \n",
       "27872       2829       2765     -753.0         2.9         3.8  0.160254   \n",
       "27873       2829       2765     -556.0         3.5         4.4  0.220721   \n",
       "27874       2829       2765    -1246.0         4.5         1.5  0.242174   \n",
       "27875       2829       2765    -1256.0         3.5         2.3  0.232750   \n",
       "27876       2829       2765    -1241.0         3.6         3.1  0.208334   \n",
       "\n",
       "           draw     black  \n",
       "27817  0.165557  0.406649  \n",
       "27818  0.207687  0.432110  \n",
       "27819  0.214405  0.413743  \n",
       "27820  0.207687  0.432110  \n",
       "27821  0.221396  0.427234  \n",
       "27822  0.165671  0.406240  \n",
       "27823  0.213038  0.417480  \n",
       "27824  0.161376  0.421633  \n",
       "27825  0.171021  0.541276  \n",
       "27826  0.124212  0.696483  \n",
       "27827  0.139174  0.306624  \n",
       "27828  0.145420  0.305785  \n",
       "27829  0.239661  0.309992  \n",
       "27830  0.202813  0.308972  \n",
       "27831  0.145162  0.305877  \n",
       "27832  0.176368  0.331235  \n",
       "27833  0.164202  0.339667  \n",
       "27834  0.152250  0.385999  \n",
       "27835  0.142768  0.312234  \n",
       "27836  0.134052  0.194817  \n",
       "27837  0.134052  0.194817  \n",
       "27838  0.139802  0.183928  \n",
       "27839  0.161759  0.291371  \n",
       "27840  0.213257  0.279245  \n",
       "27841  0.215651  0.280833  \n",
       "27842  0.215651  0.280833  \n",
       "27843  0.215318  0.281943  \n",
       "27844  0.272663  0.248695  \n",
       "27845  0.272663  0.248695  \n",
       "27846  0.259829  0.264169  \n",
       "27847  0.246561  0.301746  \n",
       "27848  0.273697  0.286218  \n",
       "27849  0.289084  0.247685  \n",
       "27850  0.284765  0.258922  \n",
       "27851  0.284765  0.258922  \n",
       "27852  0.284765  0.258922  \n",
       "27853  0.283770  0.259951  \n",
       "27854  0.283770  0.259951  \n",
       "27855  0.438185  0.232449  \n",
       "27856  0.415670  0.268483  \n",
       "27857  0.091680  0.431672  \n",
       "27858  0.091742  0.431967  \n",
       "27859  0.091455  0.432759  \n",
       "27860  0.089022  0.440241  \n",
       "27861  0.098613  0.419956  \n",
       "27862  0.107430  0.411633  \n",
       "27863  0.107950  0.415560  \n",
       "27864  0.106985  0.416009  \n",
       "27865  0.106658  0.416162  \n",
       "27866  0.150532  0.521593  \n",
       "27867  0.158505  0.586303  \n",
       "27868  0.158505  0.586303  \n",
       "27869  0.109886  0.714807  \n",
       "27870  0.123497  0.717455  \n",
       "27871  0.113287  0.700831  \n",
       "27872  0.116853  0.722893  \n",
       "27873  0.125413  0.653866  \n",
       "27874  0.073512  0.684314  \n",
       "27875  0.070929  0.696321  \n",
       "27876  0.094590  0.697076  "
      ]
     },
     "execution_count": 866,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.tail(60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 894,
   "id": "3a4f400b",
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy = accuracy_score(y_test, predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 895,
   "id": "12ee9307",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.633142037302726"
      ]
     },
     "execution_count": 895,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 896,
   "id": "977c209d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8232051862826654"
      ]
     },
     "execution_count": 896,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "log_loss(y_test, model.predict_proba(X_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 846,
   "id": "af46d717",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pandas.core.series.Series"
      ]
     },
     "execution_count": 846,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(X_test.iloc[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 856,
   "id": "dd5c46cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.22230807, 0.09929526, 0.67839664]], dtype=float32)"
      ]
     },
     "execution_count": 856,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(np.array(X_test.head(1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 855,
   "id": "38ba8c66",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2288. , 2135. ,  498. ,   86.3,   69.3]])"
      ]
     },
     "execution_count": 855,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.array(X_test.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 714,
   "id": "e6986d6d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1571"
      ]
     },
     "execution_count": 714,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "chessdotcom.client.get_tournament_round(url_id= 'early-titled-tuesday-blitz-august-02-2022-3288263/11', round_num = 1).json['tournament_round']['games']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 901,
   "id": "09639e96",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.28506163, 0.342225  , 0.3727134 ]], dtype=float32)"
      ]
     },
     "execution_count": 901,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.predict_proba(np.array([[2829, 2765, 0, 3.6, 3.1]]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
